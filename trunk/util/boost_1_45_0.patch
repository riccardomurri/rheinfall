Solo in boost_1_45_0: bin.v2
Solo in boost_1_45_0: bjam
diff -x '*~' -x '*.o' -x '*.a' -x '*.so' -wur boost_1_45_0.orig//boost/mpi/communicator.hpp boost_1_45_0/boost/mpi/communicator.hpp
--- boost_1_45_0.orig//boost/mpi/communicator.hpp	2008-06-26 21:25:44.000000000 +0200
+++ boost_1_45_0/boost/mpi/communicator.hpp	2010-11-29 01:03:11.000000000 +0100
@@ -266,9 +266,14 @@
    *  @param value The value that will be transmitted to the
    *  receiver. The type @c T of this value must meet the aforementioned
    *  criteria for transmission.
+   *
+   *  @param synchronous If @c true, use MPI "synchronous send":
+   *  guarantees that, when this call returns, the destination has
+   *  already started receiving the message.
    */
   template<typename T>
-  void send(int dest, int tag, const T& value) const;
+  void send(int dest, int tag, const T& value, 
+            const bool synchronous = false) const;
 
   /**
    *  @brief Send the skeleton of an object.
@@ -297,9 +302,13 @@
    *  @param proxy The @c skeleton_proxy containing a reference to the
    *  object whose skeleton will be transmitted.
    *
+   *  @param synchronous If @c true, use MPI "synchronous send":
+   *  guarantees that, when this call returns, the destination has
+   *  already started receiving the message.
    */
   template<typename T>
-  void send(int dest, int tag, const skeleton_proxy<T>& proxy) const;
+  void send(int dest, int tag, const skeleton_proxy<T>& proxy, 
+            const bool synchronous = false) const;
 
   /**
    *  @brief Send an array of values to another process.
@@ -327,9 +336,14 @@
    *  @param n The number of values stored in the array. The destination
    *  process must call receive with at least this many elements to
    *  correctly receive the message.
+   *
+   *  @param synchronous If @c true, use MPI "synchronous send":
+   *  guarantees that, when this call returns, the destination has
+   *  already started receiving the message.
    */
   template<typename T>
-  void send(int dest, int tag, const T* values, int n) const;
+  void send(int dest, int tag, const T* values, int n, 
+            const bool synchronous = false) const;
 
   /**
    *  @brief Send a message to another process without any data.
@@ -345,8 +359,71 @@
    *  may be any integer between zero and an implementation-defined
    *  upper limit. This limit is accessible via @c environment::max_tag().
    *
+   *  @param synchronous If @c true, use MPI "synchronous send":
+   *  guarantees that, when this call returns, the destination has
+   *  already started receiving the message.
+   */
+  void send(int dest, int tag, const bool synchronous = false) const;
+
+  /**
+   *  @brief Synchronously send data to another process.
+   *
+   *  This routine executes a potentially blocking synchronous send
+   *  with tag @p tag to the process with rank @p dest. This routine
+   *  is exactly equivalent to @ref send(int,int,const T&,const bool)
+   *  (which see) with the last argument set to @c true, thus forcing
+   *  the underlying MPI implementation to use a synchronous send
+   *  (MPI_Ssend).
+   *
    */
-  void send(int dest, int tag) const;
+  template<typename T>
+  void ssend(int dest, int tag, const T& value) const;
+
+  /**
+   *  @brief Synchronously send the skeleton of an object.
+   *
+   *  This routine executes a potentially blocking send with tag @p
+   *  tag to the process with rank @p dest. It can be received by the
+   *  destination process with a matching @c recv call. This variation
+   *  on @c ssend will be used when a synchronous send of a skeleton
+   *  is explicitly requested via code such as:
+   *
+   *  @code
+   *    comm.ssend(dest, tag, skeleton(object));
+   *  @endcode
+   *
+   *  This routine is exactly equivalent to @ref send(int,int,const
+   *  skeleton_proxy<T>&, const bool) (which see) with the last
+   *  argument set to @c true, thus forcing the underlying MPI
+   *  implementation to use a synchronous send (MPI_Ssend).
+   *
+   */
+  template<typename T>
+  void ssend(int dest, int tag, const skeleton_proxy<T>& proxy) const;
+
+  /**
+   *  @brief Synchronously send an array of values to another process.
+   *
+   *  This routine is exactly equivalent to @ref send(int,int,const
+   *  T*, const bool) (which see) with the last argument set to @c
+   *  true, thus forcing the underlying MPI implementation to use a
+   *  synchronous send (MPI_Ssend).
+   *
+   */
+  template<typename T>
+  void ssend(int dest, int tag, const T* values, int n) const;
+
+  /**
+   *  @brief Synchronously send a message to another process without
+   *  any data.
+   *
+   *  This routine is exactly equivalent to @ref send(int,int,const
+   *  bool) (which see) with the last argument set to @c true, thus
+   *  forcing the underlying MPI implementation to use a synchronous
+   *  send (MPI_Ssend).
+   *
+   */
+  void ssend(int dest, int tag) const;
 
   /**
    * @brief Receive data from a remote process.
@@ -506,10 +583,14 @@
    *  receiver. The type @c T of this value must meet the aforementioned
    *  criteria for transmission.
    *
+   *  @param synchronous If @c true, use MPI "synchronous send":
+   *  guarantees that, when this call returns, the destination has
+   *  already started receiving the message.
+   *
    *  @returns a @c request object that describes this communication.
    */
   template<typename T>
-  request isend(int dest, int tag, const T& value) const;
+  request isend(int dest, int tag, const T& value, const bool synchronous = false) const;
 
   /**
    *  @brief Send the skeleton of an object without blocking.
@@ -534,10 +615,15 @@
    *  @param proxy The @c skeleton_proxy containing a reference to the
    *  object whose skeleton will be transmitted.
    *
+   *  @param synchronous If @c true, use MPI "synchronous send":
+   *  guarantees that, when this call returns, the destination has
+   *  already started receiving the message.
+   *
    *  @returns a @c request object that describes this communication.
    */
   template<typename T>
-  request isend(int dest, int tag, const skeleton_proxy<T>& proxy) const;
+  request isend(int dest, int tag, const skeleton_proxy<T>& proxy, 
+                const bool synchronous = false) const;
 
   /**
    *  @brief Send an array of values to another process without
@@ -564,10 +650,15 @@
    *  process must call receive with at least this many elements to
    *  correctly receive the message.
    *
+   *  @param synchronous If @c true, use MPI "synchronous send":
+   *  guarantees that, when this call returns, the destination has
+   *  already started receiving the message.
+   *
    *  @returns a @c request object that describes this communication.
    */
   template<typename T>
-  request isend(int dest, int tag, const T* values, int n) const;
+  request isend(int dest, int tag, const T* values, int n,
+                const bool synchronous = false) const;
 
   /**
    *  @brief Send a message to another process without any data
@@ -586,10 +677,64 @@
    *  may be any integer between zero and an implementation-defined
    *  upper limit. This limit is accessible via @c environment::max_tag().
    *
+   *  @param synchronous If @c true, use MPI "synchronous send":
+   *  guarantees that, when this call returns, the destination has
+   *  already started receiving the message.
+   *
    *
    *  @returns a @c request object that describes this communication.
    */
-  request isend(int dest, int tag) const;
+  request isend(int dest, int tag, const bool synchronous = false) const;
+
+  /**
+   *  @brief Synchronously send a message to a remote process without
+   *  blocking.
+   *
+   *  This routine is exactly equivalent to @ref isend(int,int,const
+   *  T&, const bool) (which see) with the last argument set to @c
+   *  true, thus forcing the underlying MPI implementation to use a
+   *  synchronous send (MPI_Ssend).
+   *
+   */
+  template<typename T>
+  request issend(int dest, int tag, const T& value) const;
+
+  /**
+   *  @brief Synchronously send the skeleton of an object without blocking.
+   *
+   *  This routine is exactly equivalent to @ref isend(int,int,const
+   *  skeleton_proxy<T>&, const bool) (which see) with the last
+   *  argument set to @c true, thus forcing the underlying MPI
+   *  implementation to use a synchronous send (MPI_Ssend).
+   *
+   */
+  template<typename T>
+  request issend(int dest, int tag, const skeleton_proxy<T>& proxy) const;
+
+  /**
+   *  @brief Synchronously send an array of values to another process
+   *  without blocking.
+   *
+   *  This routine is exactly equivalent to @ref send(int,int,const
+   *  T*,int,const bool) (which see) with the last argument set to
+   *  @c true, thus forcing the underlying MPI implementation to use a
+   *  synchronous send (MPI_Ssend).
+   *
+   */
+  template<typename T>
+  request issend(int dest, int tag, const T* values, int n) const;
+
+  /**
+   *  @brief Synchronously send a message to another process without
+   *  any data without blocking.
+   *
+   *  This routine is exactly equivalent to @ref send(int,int,const
+   *  bool) (which see) with the last argument set to @c true, thus
+   *  forcing the underlying MPI implementation to use a synchronous
+   *  send (MPI_Ssend).
+   *
+   */
+  request issend(int dest, int tag) const;
 
   /**
    *  @brief Prepare to receive a message from a remote process.
@@ -885,7 +1030,8 @@
    * map directly to that datatype.
    */
   template<typename T>
-  void send_impl(int dest, int tag, const T& value, mpl::true_) const;
+  void send_impl(int dest, int tag, const T& value, mpl::true_, 
+                 const bool synchronous = false) const;
 
   /**
    * INTERNAL ONLY
@@ -895,7 +1041,8 @@
    * to be deserialized on the receiver side.
    */
   template<typename T>
-  void send_impl(int dest, int tag, const T& value, mpl::false_) const;
+  void send_impl(int dest, int tag, const T& value, mpl::false_,
+                 const bool synchronous = false) const;
 
   /**
    * INTERNAL ONLY
@@ -905,7 +1052,8 @@
    */
   template<typename T>
   void 
-  array_send_impl(int dest, int tag, const T* values, int n, mpl::true_) const;
+  array_send_impl(int dest, int tag, const T* values, int n, mpl::true_,
+                  const bool synchronous = false) const;
 
   /**
    * INTERNAL ONLY
@@ -917,7 +1065,7 @@
   template<typename T>
   void 
   array_send_impl(int dest, int tag, const T* values, int n, 
-                  mpl::false_) const;
+                  mpl::false_, const bool synchronous = false) const;
 
   /**
    * INTERNAL ONLY
@@ -926,7 +1074,8 @@
    * map directly to that datatype.
    */
   template<typename T>
-  request isend_impl(int dest, int tag, const T& value, mpl::true_) const;
+  request isend_impl(int dest, int tag, const T& value, mpl::true_,
+                     const bool synchronous = false) const;
 
   /**
    * INTERNAL ONLY
@@ -936,7 +1085,8 @@
    * to be deserialized on the receiver side.
    */
   template<typename T>
-  request isend_impl(int dest, int tag, const T& value, mpl::false_) const;
+  request isend_impl(int dest, int tag, const T& value, mpl::false_,
+                     const bool synchronous = false) const;
 
   /**
    * INTERNAL ONLY
@@ -947,7 +1097,7 @@
   template<typename T>
   request 
   array_isend_impl(int dest, int tag, const T* values, int n, 
-                   mpl::true_) const;
+                   mpl::true_, const bool synchronous = false) const;
 
   /**
    * INTERNAL ONLY
@@ -959,7 +1109,7 @@
   template<typename T>
   request 
   array_isend_impl(int dest, int tag, const T* values, int n, 
-                   mpl::false_) const;
+                   mpl::false_, const bool synchronous = false) const;
 
   /**
    * INTERNAL ONLY
@@ -1105,11 +1255,18 @@
 // map directly to that datatype.
 template<typename T>
 void
-communicator::send_impl(int dest, int tag, const T& value, mpl::true_) const
+communicator::send_impl(int dest, int tag, const T& value, mpl::true_, 
+                        const bool synchronous) const
 {
+  if (synchronous) {
+    BOOST_MPI_CHECK_RESULT(MPI_Ssend,
+                           (const_cast<T*>(&value), 1, get_mpi_datatype<T>(value),
+                            dest, tag, MPI_Comm(*this)));
+  } else {
   BOOST_MPI_CHECK_RESULT(MPI_Send,
                          (const_cast<T*>(&value), 1, get_mpi_datatype<T>(value),
                           dest, tag, MPI_Comm(*this)));
+  };
 }
 
 // We're sending a type that does not have an associated MPI
@@ -1117,19 +1274,29 @@
 // to be deserialized on the receiver side.
 template<typename T>
 void
-communicator::send_impl(int dest, int tag, const T& value, mpl::false_) const
+communicator::send_impl(int dest, int tag, const T& value, mpl::false_,
+                        const bool synchronous) const
 {
   packed_oarchive oa(*this);
   oa << value;
-  send(dest, tag, oa);
+  send(dest, tag, oa, synchronous);
+}
+
+// Single-element receive may either send the element directly or
+// serialize it via a buffer.
+template<typename T>
+void communicator::send(int dest, int tag, const T& value, 
+                        const bool synchronous) const
+{
+  this->send_impl(dest, tag, value, is_mpi_datatype<T>(), synchronous);
 }
 
 // Single-element receive may either send the element directly or
 // serialize it via a buffer.
 template<typename T>
-void communicator::send(int dest, int tag, const T& value) const
+void communicator::ssend(int dest, int tag, const T& value) const
 {
-  this->send_impl(dest, tag, value, is_mpi_datatype<T>());
+  this->send_impl(dest, tag, value, is_mpi_datatype<T>(), true);
 }
 
 // We're sending an array of a type that has an associated MPI
@@ -1137,12 +1304,19 @@
 template<typename T>
 void
 communicator::array_send_impl(int dest, int tag, const T* values, int n,
-                              mpl::true_) const
+                              mpl::true_, const bool synchronous) const
 {
+  if (synchronous) {
+    BOOST_MPI_CHECK_RESULT(MPI_Ssend,
+                           (const_cast<T*>(values), n, 
+                            get_mpi_datatype<T>(*values),
+                            dest, tag, MPI_Comm(*this)));
+  } else {
   BOOST_MPI_CHECK_RESULT(MPI_Send,
                          (const_cast<T*>(values), n, 
                           get_mpi_datatype<T>(*values),
                           dest, tag, MPI_Comm(*this)));
+  };
 }
 
 // We're sending an array of a type that does not have an associated
@@ -1151,18 +1325,26 @@
 template<typename T>
 void
 communicator::array_send_impl(int dest, int tag, const T* values, int n,
-                              mpl::false_) const
+                              mpl::false_, const bool synchronous) const
 {
   packed_oarchive oa(*this);
   oa << n << boost::serialization::make_array(values, n);
-  send(dest, tag, oa);
+  send(dest, tag, oa, synchronous);
 }
 
 // Array send must send the elements directly
 template<typename T>
-void communicator::send(int dest, int tag, const T* values, int n) const
+void communicator::send(int dest, int tag, const T* values, int n,
+                        const bool synchronous) const
+{
+  this->array_send_impl(dest, tag, values, n, is_mpi_datatype<T>(), synchronous);
+}
+
+// Array ssend must send the elements directly
+template<typename T>
+void communicator::ssend(int dest, int tag, const T* values, int n) const
 {
-  this->array_send_impl(dest, tag, values, n, is_mpi_datatype<T>());
+  this->array_send_impl(dest, tag, values, n, is_mpi_datatype<T>(), true);
 }
 
 // We're receiving a type that has an associated MPI datatype, so we
@@ -1251,13 +1433,21 @@
 // map directly to that datatype.
 template<typename T>
 request
-communicator::isend_impl(int dest, int tag, const T& value, mpl::true_) const
+communicator::isend_impl(int dest, int tag, const T& value, mpl::true_, 
+                         const bool synchronous) const
 {
   request req;
+  if (synchronous) {
+    BOOST_MPI_CHECK_RESULT(MPI_Issend,
+                           (const_cast<T*>(&value), 1, 
+                            get_mpi_datatype<T>(value),
+                            dest, tag, MPI_Comm(*this), &req.m_requests[0]));
+  } else {
   BOOST_MPI_CHECK_RESULT(MPI_Isend,
                          (const_cast<T*>(&value), 1, 
                           get_mpi_datatype<T>(value),
                           dest, tag, MPI_Comm(*this), &req.m_requests[0]));
+  };
   return req;
 }
 
@@ -1266,11 +1456,12 @@
 // to be deserialized on the receiver side.
 template<typename T>
 request
-communicator::isend_impl(int dest, int tag, const T& value, mpl::false_) const
+communicator::isend_impl(int dest, int tag, const T& value, mpl::false_,
+                         const bool synchronous) const
 {
   shared_ptr<packed_oarchive> archive(new packed_oarchive(*this));
   *archive << value;
-  request result = isend(dest, tag, *archive);
+  request result = isend(dest, tag, *archive, synchronous);
   result.m_data = archive;
   return result;
 }
@@ -1278,32 +1469,48 @@
 // Single-element receive may either send the element directly or
 // serialize it via a buffer.
 template<typename T>
-request communicator::isend(int dest, int tag, const T& value) const
+request communicator::isend(int dest, int tag, const T& value, 
+                            const bool synchronous) const
+{
+  return this->isend_impl(dest, tag, value, is_mpi_datatype<T>(), synchronous);
+}
+
+// Single-element receive may either send the element directly or
+// serialize it via a buffer.
+template<typename T>
+request communicator::issend(int dest, int tag, const T& value) const
 {
-  return this->isend_impl(dest, tag, value, is_mpi_datatype<T>());
+  return this->isend_impl(dest, tag, value, is_mpi_datatype<T>(), true);
 }
 
 template<typename T>
 request
 communicator::array_isend_impl(int dest, int tag, const T* values, int n,
-                               mpl::true_) const
+                               mpl::true_, const bool synchronous) const
 {
   request req;
+  if (synchronous) {
+    BOOST_MPI_CHECK_RESULT(MPI_Issend,
+                           (const_cast<T*>(values), n, 
+                            get_mpi_datatype<T>(*values),
+                            dest, tag, MPI_Comm(*this), &req.m_requests[0]));
+  } else {
   BOOST_MPI_CHECK_RESULT(MPI_Isend,
                          (const_cast<T*>(values), n, 
                           get_mpi_datatype<T>(*values),
                           dest, tag, MPI_Comm(*this), &req.m_requests[0]));
+  };
   return req;
 }
 
 template<typename T>
 request
 communicator::array_isend_impl(int dest, int tag, const T* values, int n, 
-                               mpl::false_) const
+                               mpl::false_, const bool synchronous) const
 {
   shared_ptr<packed_oarchive> archive(new packed_oarchive(*this));
   *archive << n << boost::serialization::make_array(values, n);
-  request result = isend(dest, tag, *archive);
+  request result = isend(dest, tag, *archive, synchronous);
   result.m_data = archive;
   return result;
 }
@@ -1311,9 +1518,17 @@
 
 // Array isend must send the elements directly
 template<typename T>
-request communicator::isend(int dest, int tag, const T* values, int n) const
+request communicator::isend(int dest, int tag, const T* values, int n,
+                            const bool synchronous) const
+{
+  return array_isend_impl(dest, tag, values, n, is_mpi_datatype<T>(), synchronous);
+}
+
+// Array isend must send the elements directly
+template<typename T>
+request communicator::issend(int dest, int tag, const T* values, int n) const
 {
-  return array_isend_impl(dest, tag, values, n, is_mpi_datatype<T>());
+  return array_isend_impl(dest, tag, values, n, is_mpi_datatype<T>(), true);
 }
 
 namespace detail {
@@ -1607,7 +1822,8 @@
 template<>
 BOOST_MPI_DECL void
 communicator::send<packed_oarchive>(int dest, int tag,
-                                    const packed_oarchive& ar) const;
+                                    const packed_oarchive& ar,
+                                    const bool synchronous) const;
 
 /**
  * INTERNAL ONLY
@@ -1615,14 +1831,17 @@
 template<>
 BOOST_MPI_DECL void
 communicator::send<packed_skeleton_oarchive>
-  (int dest, int tag, const packed_skeleton_oarchive& ar) const;
+                                   (int dest, int tag, 
+                                    const packed_skeleton_oarchive& ar,
+                                    const bool synchronous) const;
 
 /**
  * INTERNAL ONLY
  */
 template<>
 BOOST_MPI_DECL void
-communicator::send<content>(int dest, int tag, const content& c) const;
+communicator::send<content>(int dest, int tag, const content& c,
+                            const bool synchronous) const;
 
 /**
  * INTERNAL ONLY
@@ -1665,7 +1884,8 @@
 template<>
 BOOST_MPI_DECL request
 communicator::isend<packed_oarchive>(int dest, int tag,
-                                     const packed_oarchive& ar) const;
+                                     const packed_oarchive& ar,
+                                     const bool synchronous) const;
 
 /**
  * INTERNAL ONLY
@@ -1673,14 +1893,17 @@
 template<>
 BOOST_MPI_DECL request
 communicator::isend<packed_skeleton_oarchive>
-  (int dest, int tag, const packed_skeleton_oarchive& ar) const;
+                                    (int dest, int tag, 
+                                     const packed_skeleton_oarchive& ar, 
+                                     const bool synchronous) const;
 
 /**
  * INTERNAL ONLY
  */
 template<>
 BOOST_MPI_DECL request
-communicator::isend<content>(int dest, int tag, const content& c) const;
+communicator::isend<content>(int dest, int tag, const content& c, 
+                             const bool synchronous) const;
 
 /**
  * INTERNAL ONLY
diff -x '*~' -x '*.o' -x '*.a' -x '*.so' -wur boost_1_45_0.orig//boost/mpi/detail/communicator_sc.hpp boost_1_45_0/boost/mpi/detail/communicator_sc.hpp
--- boost_1_45_0.orig//boost/mpi/detail/communicator_sc.hpp	2007-11-25 19:07:19.000000000 +0100
+++ boost_1_45_0/boost/mpi/detail/communicator_sc.hpp	2010-11-29 01:01:12.000000000 +0100
@@ -15,11 +15,12 @@
 
 template<typename T>
 void
-communicator::send(int dest, int tag, const skeleton_proxy<T>& proxy) const
+communicator::send(int dest, int tag, const skeleton_proxy<T>& proxy,
+                   const bool synchronous) const
 {
   packed_skeleton_oarchive ar(*this);
   ar << proxy.object;
-  send(dest, tag, ar);
+  send(dest, tag, ar, synchronous);
 }
 
 template<typename T>
@@ -43,13 +44,14 @@
 
 template<typename T>
 request
-communicator::isend(int dest, int tag, const skeleton_proxy<T>& proxy) const
+communicator::isend(int dest, int tag, const skeleton_proxy<T>& proxy,
+                    const bool synchronous) const
 {
   shared_ptr<packed_skeleton_oarchive> 
     archive(new packed_skeleton_oarchive(*this));
 
   *archive << proxy.object;
-  request result = isend(dest, tag, *archive);
+  request result = isend(dest, tag, *archive, synchronous);
   result.m_data = archive;
   return result;
 }
diff -x '*~' -x '*.o' -x '*.a' -x '*.so' -wur boost_1_45_0.orig//boost/mpi/detail/point_to_point.hpp boost_1_45_0/boost/mpi/detail/point_to_point.hpp
--- boost_1_45_0.orig//boost/mpi/detail/point_to_point.hpp	2007-11-25 19:07:19.000000000 +0100
+++ boost_1_45_0/boost/mpi/detail/point_to_point.hpp	2010-11-29 01:01:11.000000000 +0100
@@ -15,12 +15,13 @@
 
 namespace boost { namespace mpi { namespace detail {
 
-/** Sends a packed archive using MPI_Send. */
+/** Sends a packed archive using MPI_Send/MPI_Ssend. */
 BOOST_MPI_DECL void
 packed_archive_send(MPI_Comm comm, int dest, int tag,
-                    const packed_oarchive& ar);
+                    const packed_oarchive& ar,
+                    const bool synchronous = false);
 
-/** Sends a packed archive using MPI_Isend.
+/** Sends a packed archive using MPI_Isend/MPI_Issend.
  *
  * This routine may split sends into multiple packets. The MPI_Request
  * for each packet will be placed into the out_requests array, up to
@@ -32,7 +33,8 @@
 BOOST_MPI_DECL int
 packed_archive_isend(MPI_Comm comm, int dest, int tag,
                      const packed_oarchive& ar,
-                     MPI_Request* out_requests, int num_out_requests);
+                     MPI_Request* out_requests, int num_out_requests,
+                     const bool synchronous = false);
 
 /**
  * \overload
@@ -40,7 +42,8 @@
 BOOST_MPI_DECL int
 packed_archive_isend(MPI_Comm comm, int dest, int tag,
                      const packed_iarchive& ar,
-                     MPI_Request* out_requests, int num_out_requests);
+                     MPI_Request* out_requests, int num_out_requests,
+                     const bool synchronous = false);
 
 /** Receives a packed archive using MPI_Recv. */
 BOOST_MPI_DECL void
Solo in boost_1_45_0: bootstrap.log
diff -x '*~' -x '*.o' -x '*.a' -x '*.so' -wur boost_1_45_0.orig//libs/mpi/src/broadcast.cpp boost_1_45_0/libs/mpi/src/broadcast.cpp
--- boost_1_45_0.orig//libs/mpi/src/broadcast.cpp	2007-11-25 19:38:02.000000000 +0100
+++ boost_1_45_0/libs/mpi/src/broadcast.cpp	2010-11-29 00:27:21.000000000 +0100
@@ -35,7 +35,7 @@
     if (dest != root) {
       // Build up send requests for each child send.
       num_requests += detail::packed_archive_isend(comm, dest, tag, oa,
-                                                   &requests[num_requests], 2);
+                                                   &requests[num_requests], 2, false);
     }
   }
 
@@ -76,7 +76,7 @@
         // Build up send requests for each child send.
         num_requests += detail::packed_archive_isend(comm, dest, tag, ia,
                                                      &requests[num_requests],
-                                                     2);
+                                                     2, false);
       }
     }
 
diff -x '*~' -x '*.o' -x '*.a' -x '*.so' -wur boost_1_45_0.orig//libs/mpi/src/communicator.cpp boost_1_45_0/libs/mpi/src/communicator.cpp
--- boost_1_45_0.orig//libs/mpi/src/communicator.cpp	2007-11-25 19:38:02.000000000 +0100
+++ boost_1_45_0/libs/mpi/src/communicator.cpp	2010-11-29 00:27:19.000000000 +0100
@@ -87,11 +87,17 @@
   return boost::mpi::group(gr, /*adopt=*/true);
 }
 
-void communicator::send(int dest, int tag) const
+    void communicator::send(int dest, int tag, const bool synchronous) const
 {
+  if (synchronous) {
+    BOOST_MPI_CHECK_RESULT(MPI_Ssend,
+                           (MPI_BOTTOM, 0, MPI_PACKED,
+                            dest, tag, MPI_Comm(*this)));
+  } else {
   BOOST_MPI_CHECK_RESULT(MPI_Send,
                          (MPI_BOTTOM, 0, MPI_PACKED,
                           dest, tag, MPI_Comm(*this)));
+  };
 }
 
 status communicator::recv(int source, int tag) const
@@ -190,25 +196,33 @@
 template<>
 void
 communicator::send<packed_oarchive>(int dest, int tag,
-                                    const packed_oarchive& ar) const
+                                    const packed_oarchive& ar,
+                                    const bool synchronous) const
 {
-  detail::packed_archive_send(MPI_Comm(*this), dest, tag, ar);
+  detail::packed_archive_send(MPI_Comm(*this), dest, tag, ar, synchronous);
 }
 
 template<>
 void
 communicator::send<packed_skeleton_oarchive>
-  (int dest, int tag, const packed_skeleton_oarchive& ar) const
+(int dest, int tag, const packed_skeleton_oarchive& ar, const bool synchronous) const
 {
-  this->send(dest, tag, ar.get_skeleton());
+  this->send(dest, tag, ar.get_skeleton(), synchronous);
 }
 
 template<>
-void communicator::send<content>(int dest, int tag, const content& c) const
+void communicator::send<content>(int dest, int tag, const content& c,
+                                 const bool synchronous) const
 {
+  if (synchronous) {
+    BOOST_MPI_CHECK_RESULT(MPI_Ssend,
+                           (MPI_BOTTOM, 1, c.get_mpi_datatype(),
+                            dest, tag, MPI_Comm(*this)));
+  } else {
   BOOST_MPI_CHECK_RESULT(MPI_Send,
                          (MPI_BOTTOM, 1, c.get_mpi_datatype(),
                           dest, tag, MPI_Comm(*this)));
+  };
 }
 
 template<>
@@ -247,38 +261,53 @@
 template<>
 request
 communicator::isend<packed_oarchive>(int dest, int tag,
-                                     const packed_oarchive& ar) const
+                                     const packed_oarchive& ar,
+                                     const bool synchronous) const
 {
   request req;
   detail::packed_archive_isend(MPI_Comm(*this), dest, tag, ar,
-                               &req.m_requests[0] ,2);
+                               &req.m_requests[0], 2, synchronous);
   return req;
 }
 
 template<>
 request
 communicator::isend<packed_skeleton_oarchive>
-  (int dest, int tag, const packed_skeleton_oarchive& ar) const
+(int dest, int tag, const packed_skeleton_oarchive& ar,
+ const bool synchronous) const
 {
-  return this->isend(dest, tag, ar.get_skeleton());
+  return this->isend(dest, tag, ar.get_skeleton(), synchronous);
 }
 
 template<>
-request communicator::isend<content>(int dest, int tag, const content& c) const
+request communicator::isend<content>(int dest, int tag, const content& c,
+                                     const bool synchronous) const
 {
   request req;
+  if (synchronous) {
+    BOOST_MPI_CHECK_RESULT(MPI_Issend,
+                           (MPI_BOTTOM, 1, c.get_mpi_datatype(),
+                            dest, tag, MPI_Comm(*this), &req.m_requests[0]));
+  } else {
   BOOST_MPI_CHECK_RESULT(MPI_Isend,
                          (MPI_BOTTOM, 1, c.get_mpi_datatype(),
                           dest, tag, MPI_Comm(*this), &req.m_requests[0]));
+  };
   return req;
 }
 
-request communicator::isend(int dest, int tag) const
+request communicator::isend(int dest, int tag, const bool synchronous) const
 {
   request req;
+  if (synchronous) {
+    BOOST_MPI_CHECK_RESULT(MPI_Issend,
+                           (MPI_BOTTOM, 0, MPI_PACKED,
+                            dest, tag, MPI_Comm(*this), &req.m_requests[0]));
+  } else {
   BOOST_MPI_CHECK_RESULT(MPI_Isend,
                          (MPI_BOTTOM, 0, MPI_PACKED,
                           dest, tag, MPI_Comm(*this), &req.m_requests[0]));
+  };
   return req;
 }
 
diff -x '*~' -x '*.o' -x '*.a' -x '*.so' -wur boost_1_45_0.orig//libs/mpi/src/point_to_point.cpp boost_1_45_0/libs/mpi/src/point_to_point.cpp
--- boost_1_45_0.orig//libs/mpi/src/point_to_point.cpp	2008-06-26 21:25:44.000000000 +0200
+++ boost_1_45_0/libs/mpi/src/point_to_point.cpp	2010-11-29 00:27:19.000000000 +0100
@@ -26,34 +26,58 @@
 
 void
 packed_archive_send(MPI_Comm comm, int dest, int tag,
-                    const packed_oarchive& ar)
+                    const packed_oarchive& ar,
+                    const bool synchronous)
 {
   const void* size = &ar.size();
+  // size and content are sent in separate messages: since we only
+  // need a guarantee that the *content* has arrived when a
+  // synchronous call is done, we can send the content with MPI_Ssend
+  // and use a "standard send" for the size.
   BOOST_MPI_CHECK_RESULT(MPI_Send,
                          (const_cast<void*>(size), 1, 
                           get_mpi_datatype<std::size_t>(ar.size()), 
                           dest, tag, comm));
+  if (synchronous) {
+    BOOST_MPI_CHECK_RESULT(MPI_Ssend,
+                           (const_cast<void*>(ar.address()), ar.size(),
+                            MPI_PACKED,
+                            dest, tag, comm));
+  } else {
   BOOST_MPI_CHECK_RESULT(MPI_Send,
                          (const_cast<void*>(ar.address()), ar.size(),
                           MPI_PACKED,
                           dest, tag, comm));
+  };
 }
 
 int
 packed_archive_isend(MPI_Comm comm, int dest, int tag,
                      const packed_oarchive& ar,
-                     MPI_Request* out_requests, int num_out_requests)
+                     MPI_Request* out_requests, int num_out_requests,
+                     const bool synchronous)
 {
   assert(num_out_requests >= 2);
   const void* size = &ar.size();
+  // size and content are sent in separate messages: since we only
+  // need a guarantee that the *content* has arrived when a
+  // synchronous call is done, we can send the content with MPI_Ssend
+  // and use a "standard send" for the size.
   BOOST_MPI_CHECK_RESULT(MPI_Isend,
                          (const_cast<void*>(size), 1, 
                           get_mpi_datatype<std::size_t>(ar.size()), 
                           dest, tag, comm, out_requests));
+  if (synchronous) {
+    BOOST_MPI_CHECK_RESULT(MPI_Issend,
+                           (const_cast<void*>(ar.address()), ar.size(),
+                            MPI_PACKED,
+                            dest, tag, comm, out_requests + 1));
+  } else {
   BOOST_MPI_CHECK_RESULT(MPI_Isend,
                          (const_cast<void*>(ar.address()), ar.size(),
                           MPI_PACKED,
                           dest, tag, comm, out_requests + 1));
+  };
 
   return 2;
 }
@@ -61,19 +85,31 @@
 int
 packed_archive_isend(MPI_Comm comm, int dest, int tag,
                      const packed_iarchive& ar,
-                     MPI_Request* out_requests, int num_out_requests)
+                     MPI_Request* out_requests, int num_out_requests,
+                     const bool synchronous)
 {
   assert(num_out_requests >= 2);
 
   const void* size = &ar.size();
+  // size and content are sent in separate messages: since we only
+  // need a guarantee that the *content* has arrived when a
+  // synchronous call is done, we can send the content with MPI_Ssend
+  // and use a "standard send" for the size.
   BOOST_MPI_CHECK_RESULT(MPI_Isend,
                          (const_cast<void*>(size), 1, 
                           get_mpi_datatype<std::size_t>(ar.size()), 
                           dest, tag, comm, out_requests));
+  if (synchronous) {
+    BOOST_MPI_CHECK_RESULT(MPI_Issend,
+                           (const_cast<void*>(ar.address()), ar.size(),
+                            MPI_PACKED,
+                            dest, tag, comm, out_requests + 1));
+  } else {
   BOOST_MPI_CHECK_RESULT(MPI_Isend,
                          (const_cast<void*>(ar.address()), ar.size(),
                           MPI_PACKED,
                           dest, tag, comm, out_requests + 1));
+  };
 
   return 2;
 }
diff -x '*~' -x '*.o' -x '*.a' -x '*.so' -wur boost_1_45_0.orig//libs/mpi/test/nonblocking_test.cpp boost_1_45_0/libs/mpi/test/nonblocking_test.cpp
--- boost_1_45_0.orig//libs/mpi/test/nonblocking_test.cpp	2007-11-25 19:38:02.000000000 +0100
+++ boost_1_45_0/libs/mpi/test/nonblocking_test.cpp	2010-11-29 00:41:16.000000000 +0100
@@ -41,11 +41,14 @@
   "test_some (keep results)"
 };
 
+enum synch_behavior { synch_no, synch_yes, synch_both };
+
 
 template<typename T>
 void
 nonblocking_test(const communicator& comm, const T* values, int num_values, 
-                 const char* kind, method_kind method = mk_all)
+                 const char* kind, method_kind method = mk_all, 
+                 synch_behavior sb = synch_both)
 {
   using boost::mpi::wait_any;
   using boost::mpi::test_any;
@@ -54,6 +57,12 @@
   using boost::mpi::wait_some;
   using boost::mpi::test_some;
 
+  if (sb == synch_both) {
+    nonblocking_test(comm, values, num_values, kind, method, synch_no);
+    nonblocking_test(comm, values, num_values, kind, method, synch_yes);
+    return;
+  };
+
   if (method == mk_all || method == mk_all_except_test_all) {
     nonblocking_test(comm, values, num_values, kind, mk_wait_any);
     nonblocking_test(comm, values, num_values, kind, mk_test_any);
@@ -68,9 +77,13 @@
     nonblocking_test(comm, values, num_values, kind, mk_test_some);
     nonblocking_test(comm, values, num_values, kind, mk_test_some_keep);
   } else {
+    const bool synchronous = (sb == synch_yes);
+
     if (comm.rank() == 0) {
       std::cout << "Testing " << method_kind_names[method] 
-                << " with " << kind << "...";
+                << " with " << kind 
+                << (synchronous ? " (ssend) " : "")
+                << "...";
       std::cout.flush();
     }
 
@@ -82,7 +95,7 @@
 
     std::vector<request> reqs;
     // Send/receive the first value
-    reqs.push_back(comm.isend((comm.rank() + 1) % comm.size(), 0, values[0]));
+    reqs.push_back(comm.isend((comm.rank() + 1) % comm.size(), 0, values[0], synchronous));
     reqs.push_back(comm.irecv((comm.rank() + comm.size() - 1) % comm.size(),
                               0, incoming_value));
 
@@ -94,14 +107,14 @@
       // when using shared memory, not TCP.
 
       // Send/receive an empty message
-      reqs.push_back(comm.isend((comm.rank() + 1) % comm.size(), 1));
+      reqs.push_back(comm.isend((comm.rank() + 1) % comm.size(), 1, synchronous));
       reqs.push_back(comm.irecv((comm.rank() + comm.size() - 1) % comm.size(),
                                 1));
 #endif
 
       // Send/receive an array
       reqs.push_back(comm.isend((comm.rank() + 1) % comm.size(), 2, values,
-                                num_values));
+                                num_values, synchronous));
       reqs.push_back(comm.irecv((comm.rank() + comm.size() - 1) % comm.size(),
                                 2, &incoming_values.front(), num_values));
     }
diff -x '*~' -x '*.o' -x '*.a' -x '*.so' -wur boost_1_45_0.orig//libs/mpi/test/skeleton_content_test.cpp boost_1_45_0/libs/mpi/test/skeleton_content_test.cpp
--- boost_1_45_0.orig//libs/mpi/test/skeleton_content_test.cpp	2007-11-25 19:38:02.000000000 +0100
+++ boost_1_45_0/libs/mpi/test/skeleton_content_test.cpp	2010-11-29 01:20:07.000000000 +0100
@@ -23,7 +23,7 @@
 
 void
 test_skeleton_and_content(const communicator& comm, int root,
-                          bool manual_broadcast)
+                          bool manual_broadcast, bool synchronous)
 {
   using boost::mpi::skeleton;
   using boost::mpi::content;
@@ -41,11 +41,12 @@
       original_list.push_back(i);
 
     std::cout << "Broadcasting integer list skeleton from root " << root
+              << (synchronous and manual_broadcast? " (ssend) " : "") 
               << "...";
     if (manual_broadcast) {
       // Broadcast the skeleton (manually)
       for (int p = 0; p < comm.size(); ++p)
-        if (p != root) comm.send(p, 0, skeleton(original_list));
+        if (p != root) comm.send(p, 0, skeleton(original_list), synchronous);
     } else {
       broadcast(comm, skeleton(original_list), root);
     }
@@ -53,22 +54,22 @@
 
     // Broadcast the content (manually)
     std::cout << "Broadcasting integer list content from root " << root
-              << "...";
+              << (synchronous? " (ssend) " : "") << "...";
     {
       content c = get_content(original_list);
       for (int p = 0; p < comm.size(); ++p)
-        if (p != root) comm.send(p, 1, c);
+        if (p != root) comm.send(p, 1, c, synchronous);
     }
     std::cout << "OK." << std::endl;
 
     // Reverse the list, broadcast the content again
     std::reverse(original_list.begin(), original_list.end());
     std::cout << "Broadcasting reversed integer list content from root "
-              << root << "...";
+              << root << (synchronous? " (ssend) " : "") << "...";
     {
       content c = get_content(original_list);
       for (int p = 0; p < comm.size(); ++p)
-        if (p != root) comm.send(p, 2, c);
+        if (p != root) comm.send(p, 2, c, synchronous);
     }
     std::cout << "OK." << std::endl;
   } else {
@@ -101,8 +102,10 @@
   (comm.barrier)();
 }
 
+
 void
-test_skeleton_and_content_nonblocking(const communicator& comm, int root)
+test_skeleton_and_content_nonblocking(const communicator& comm, int root, 
+                                      bool synchronous)
 {
   using boost::mpi::skeleton;
   using boost::mpi::content;
@@ -122,6 +125,7 @@
       original_list.push_back(i);
 
     std::cout << "Non-blocking broadcast of integer list skeleton from root " << root
+              << (synchronous? " (ssend) " : "") 
               << "...";
 
     // Broadcast the skeleton (manually)
@@ -129,19 +133,20 @@
       std::vector<request> reqs;
       for (int p = 0; p < comm.size(); ++p)
         if (p != root) 
-          reqs.push_back(comm.isend(p, 0, skeleton(original_list)));
+          reqs.push_back(comm.isend(p, 0, skeleton(original_list), synchronous));
       wait_all(reqs.begin(), reqs.end());
     }
     std::cout << "OK." << std::endl;
 
     // Broadcast the content (manually)
     std::cout << "Non-blocking broadcast of integer list content from root " << root
+              << (synchronous? " (ssend) " : "") 
               << "...";
     {
       content c = get_content(original_list);
       std::vector<request> reqs;
       for (int p = 0; p < comm.size(); ++p)
-        if (p != root) reqs.push_back(comm.isend(p, 1, c));
+        if (p != root) reqs.push_back(comm.isend(p, 1, c, synchronous));
       wait_all(reqs.begin(), reqs.end());
     }
     std::cout << "OK." << std::endl;
@@ -149,12 +154,13 @@
     // Reverse the list, broadcast the content again
     std::reverse(original_list.begin(), original_list.end());
     std::cout << "Non-blocking broadcast of reversed integer list content from root "
+              << (synchronous? " (ssend) " : "") 
               << root << "...";
     {
       std::vector<request> reqs;
       content c = get_content(original_list);
       for (int p = 0; p < comm.size(); ++p)
-        if (p != root) reqs.push_back(comm.isend(p, 2, c));
+        if (p != root) reqs.push_back(comm.isend(p, 2, c, synchronous));
       wait_all(reqs.begin(), reqs.end());
     }
     std::cout << "OK." << std::endl;
@@ -199,12 +205,21 @@
     MPI_Abort(comm, -1);
   }
 
-  test_skeleton_and_content(comm, 0, true);
-  test_skeleton_and_content(comm, 0, false);
-  test_skeleton_and_content(comm, 1, true);
-  test_skeleton_and_content(comm, 1, false);
-  test_skeleton_and_content_nonblocking(comm, 0);
-  test_skeleton_and_content_nonblocking(comm, 1);
+  // non-synchronous
+  test_skeleton_and_content(comm, 0, true, false);
+  test_skeleton_and_content(comm, 0, false, false);
+  test_skeleton_and_content(comm, 1, true, false);
+  test_skeleton_and_content(comm, 1, false, false);
+  // synchronous
+  test_skeleton_and_content(comm, 0, true, true);
+  test_skeleton_and_content(comm, 0, false, true);
+  test_skeleton_and_content(comm, 1, true, true);
+  test_skeleton_and_content(comm, 1, false, true);
+
+  test_skeleton_and_content_nonblocking(comm, 0, false);
+  test_skeleton_and_content_nonblocking(comm, 1, false);
+  test_skeleton_and_content_nonblocking(comm, 0, true);
+  test_skeleton_and_content_nonblocking(comm, 1, true);
 
   return 0;
 }
Solo in boost_1_45_0: project-config.jam
Solo in boost_1_45_0: stage
Solo in boost_1_45_0/tools/build/v2/engine/src: bin.linuxx86_64
Solo in boost_1_45_0/tools/build/v2/engine/src: bootstrap
